{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7aDTCTd8WbN",
        "outputId": "5871627a-1694-46ed-989f-e776336a76e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LMNMhj08URI",
        "outputId": "bf5bccff-4dd6-410e-bcee-18054d0dbbe9"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparing dataset...\n",
            "Training and comparing models...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n",
            "100%|██████████| 30.8M/30.8M [00:00<00:00, 75.9MB/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CBAMWDnet - Epoch 1, Loss: 0.6192\n",
            "CBAMWDnet - Epoch 2, Loss: 0.4057\n",
            "CBAMWDnet - Epoch 3, Loss: 0.2577\n",
            "CBAMWDnet - Epoch 4, Loss: 0.1745\n",
            "CBAMWDnet - Epoch 5, Loss: 0.1013\n",
            "CBAMWDnet - Epoch 6, Loss: 0.1152\n",
            "CBAMWDnet - Epoch 7, Loss: 0.0680\n",
            "CBAMWDnet - Epoch 8, Loss: 0.0248\n",
            "CBAMWDnet - Epoch 9, Loss: 0.0362\n",
            "CBAMWDnet - Epoch 10, Loss: 0.0341\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DenseNet121 - Epoch 1, Loss: 0.5247\n",
            "DenseNet121 - Epoch 2, Loss: 0.3073\n",
            "DenseNet121 - Epoch 3, Loss: 0.1944\n",
            "DenseNet121 - Epoch 4, Loss: 0.1354\n",
            "DenseNet121 - Epoch 5, Loss: 0.0566\n",
            "DenseNet121 - Epoch 6, Loss: 0.0554\n",
            "DenseNet121 - Epoch 7, Loss: 0.0504\n",
            "DenseNet121 - Epoch 8, Loss: 0.0409\n",
            "DenseNet121 - Epoch 9, Loss: 0.0184\n",
            "DenseNet121 - Epoch 10, Loss: 0.0180\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 153MB/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ResNet50 - Epoch 1, Loss: 0.4635\n",
            "ResNet50 - Epoch 2, Loss: 0.2275\n",
            "ResNet50 - Epoch 3, Loss: 0.1178\n",
            "ResNet50 - Epoch 4, Loss: 0.1010\n",
            "ResNet50 - Epoch 5, Loss: 0.0636\n",
            "ResNet50 - Epoch 6, Loss: 0.0544\n",
            "ResNet50 - Epoch 7, Loss: 0.0516\n",
            "ResNet50 - Epoch 8, Loss: 0.0304\n",
            "ResNet50 - Epoch 9, Loss: 0.0417\n",
            "ResNet50 - Epoch 10, Loss: 0.0189\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:07<00:00, 70.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGG16 - Epoch 1, Loss: 0.5287\n",
            "VGG16 - Epoch 2, Loss: 0.3789\n",
            "VGG16 - Epoch 3, Loss: 0.3270\n",
            "VGG16 - Epoch 4, Loss: 0.2521\n",
            "VGG16 - Epoch 5, Loss: 0.2334\n",
            "VGG16 - Epoch 6, Loss: 0.1955\n",
            "VGG16 - Epoch 7, Loss: 0.1770\n",
            "VGG16 - Epoch 8, Loss: 0.0865\n",
            "VGG16 - Epoch 9, Loss: 0.1164\n",
            "VGG16 - Epoch 10, Loss: 0.0999\n",
            "\n",
            "=== Model Comparison ===\n",
            "Model        Accuracy   Precision  Recall     F1        \n",
            "CBAMWDnet    86.88      0.90       0.79       0.84      \n",
            "DenseNet121  86.25      0.84       0.86       0.85      \n",
            "ResNet50     86.88      0.92       0.78       0.84      \n",
            "VGG16        83.75      0.80       0.85       0.82      \n"
          ]
        }
      ],
      "source": [
        "# CBAMWDnet TB Detection - Full Pipeline with Model Comparison\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# === Step 1: Prepare Dataset ===\n",
        "\n",
        "def prepare_dataset():\n",
        "    MONT_PATH = \"/content/drive/MyDrive/DataSet/MontgomerySet/CXR_png\"\n",
        "    SHENZHEN_PATH = \"/content/drive/MyDrive/DataSet/ChinaSet_AllFiles/CXR_png\"\n",
        "    TBX11K_PATH = \"/content/drive/MyDrive/TB_Dataset/TBnNormal\"\n",
        "\n",
        "    OUTPUT_PATH = \"/content/filtered_dataset\"\n",
        "    TB_DIR = os.path.join(OUTPUT_PATH, \"TB\")\n",
        "    NORMAL_DIR = os.path.join(OUTPUT_PATH, \"Normal\")\n",
        "\n",
        "    os.makedirs(TB_DIR, exist_ok=True)\n",
        "    os.makedirs(NORMAL_DIR, exist_ok=True)\n",
        "\n",
        "    def copy_resize(img_path, label, count):\n",
        "        target_dir = TB_DIR if label == \"TB\" else NORMAL_DIR\n",
        "        try:\n",
        "            img = Image.open(img_path)\n",
        "            if img.mode != 'RGB':\n",
        "                img = img.convert('RGB')\n",
        "            img = img.resize((224, 224))\n",
        "            img.save(os.path.join(target_dir, f\"{label}_{count}.jpg\"), format='JPEG')\n",
        "        except Exception as e:\n",
        "            print(f\"Skipped {img_path}: {e}\")\n",
        "\n",
        "    image_records = []\n",
        "\n",
        "    for img in Path(MONT_PATH).rglob(\"*.png\"):\n",
        "        name = img.name\n",
        "        if \"_0.png\" in name:\n",
        "            label = \"Normal\"\n",
        "        elif \"_1.png\" in name:\n",
        "            label = \"TB\"\n",
        "        else:\n",
        "            continue\n",
        "        image_records.append((img, label))\n",
        "\n",
        "    for img in Path(SHENZHEN_PATH).rglob(\"*.png\"):\n",
        "        name = img.name\n",
        "        if \"_0.png\" in name:\n",
        "            label = \"Normal\"\n",
        "        elif \"_1.png\" in name:\n",
        "            label = \"TB\"\n",
        "        else:\n",
        "            continue\n",
        "        image_records.append((img, label))\n",
        "\n",
        "    tbx_tb_path = os.path.join(TBX11K_PATH, \"PULMONARY_TUBERCULOSIS\")\n",
        "    tbx_normal_path = os.path.join(TBX11K_PATH, \"NORMAL\")\n",
        "\n",
        "    for img in Path(tbx_tb_path).rglob(\"*.jpg\"):\n",
        "        image_records.append((img, \"TB\"))\n",
        "    for img in Path(tbx_normal_path).rglob(\"*.jpg\"):\n",
        "        image_records.append((img, \"Normal\"))\n",
        "\n",
        "    random.shuffle(image_records)\n",
        "    tb_images = [img for img in image_records if img[1] == \"TB\"][:2500]\n",
        "    normal_images = [img for img in image_records if img[1] == \"Normal\"][:2500]\n",
        "\n",
        "    for i, (img_path, label) in enumerate(tb_images + normal_images):\n",
        "        copy_resize(img_path, label, i)\n",
        "\n",
        "# === Step 2: Dataset Loader ===\n",
        "\n",
        "def get_dataloaders(data_dir, batch_size=32):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    dataset = datasets.ImageFolder(data_dir, transform=transform)\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    test_size = len(dataset) - train_size\n",
        "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "# === Step 3: CBAMWDnet Model ===\n",
        "\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, in_planes, reduction=16):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Conv2d(in_planes, in_planes // reduction, 1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_planes // reduction, in_planes, 1, bias=False)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = self.fc(self.avg_pool(x))\n",
        "        max_out = self.fc(self.max_pool(x))\n",
        "        return self.sigmoid(avg_out + max_out)\n",
        "\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "        padding = kernel_size // 2\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        x = torch.cat([avg, max_], dim=1)\n",
        "        return self.sigmoid(self.conv(x))\n",
        "\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, in_planes, reduction=16, kernel_size=7):\n",
        "        super(CBAM, self).__init__()\n",
        "        self.ca = ChannelAttention(in_planes, reduction)\n",
        "        self.sa = SpatialAttention(kernel_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x * self.ca(x)\n",
        "        x = x * self.sa(x)\n",
        "        return x\n",
        "\n",
        "class CBAMWDnet(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(CBAMWDnet, self).__init__()\n",
        "        base_model = models.densenet121(pretrained=True)\n",
        "        self.features = base_model.features  # no conv0 modification\n",
        "        self.cbam = CBAM(1024)               # Apply CBAM at the end of DenseNet features\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(1024, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.cbam(x)\n",
        "        return self.classifier(x)\n",
        "\n",
        "# === Step 4: Train, Evaluate, and Compare ===\n",
        "\n",
        "def train_model(model, train_loader, test_loader, device, model_name):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "    train_losses = []\n",
        "\n",
        "    for epoch in range(10):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        train_losses.append(avg_loss)\n",
        "        print(f\"{model_name} - Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in test_loader:\n",
        "            imgs = imgs.to(device)\n",
        "            outputs = model(imgs)\n",
        "            preds = torch.argmax(outputs, dim=1).cpu()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels)\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds)\n",
        "    recall = recall_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds)\n",
        "\n",
        "    # Save confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Normal\", \"TB\"], yticklabels=[\"Normal\", \"TB\"])\n",
        "    plt.title(f\"Confusion Matrix - {model_name}\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.savefig(f\"{model_name}_confusion_matrix.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # Plot training loss\n",
        "    plt.figure()\n",
        "    plt.plot(train_losses, 'bo-', label='Loss')\n",
        "    plt.title(f\"Training Loss - {model_name}\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig(f\"{model_name}_training_loss.png\")\n",
        "    plt.close()\n",
        "\n",
        "    return model_name, acc, precision, recall, f1\n",
        "\n",
        "def train_and_compare():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    train_loader, test_loader = get_dataloaders(\"/content/filtered_dataset\", batch_size=32)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    model = CBAMWDnet(num_classes=2).to(device)\n",
        "    results.append(train_model(model, train_loader, test_loader, device, \"CBAMWDnet\"))\n",
        "\n",
        "    base_model = models.densenet121(pretrained=True)\n",
        "    base_model.classifier = nn.Linear(1024, 2)\n",
        "    results.append(train_model(base_model.to(device), train_loader, test_loader, device, \"DenseNet121\"))\n",
        "\n",
        "    base_model = models.resnet50(pretrained=True)\n",
        "    base_model.fc = nn.Linear(2048, 2)\n",
        "    results.append(train_model(base_model.to(device), train_loader, test_loader, device, \"ResNet50\"))\n",
        "\n",
        "    base_model = models.vgg16(pretrained=True)\n",
        "    base_model.classifier[6] = nn.Linear(4096, 2)\n",
        "    results.append(train_model(base_model.to(device), train_loader, test_loader, device, \"VGG16\"))\n",
        "\n",
        "    print(\"\\n=== Model Comparison ===\")\n",
        "    print(\"{:<12} {:<10} {:<10} {:<10} {:<10}\".format(\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\"))\n",
        "    for name, acc, prec, rec, f1 in results:\n",
        "        print(f\"{name:<12} {acc*100:<10.2f} {prec:<10.2f} {rec:<10.2f} {f1:<10.2f}\")\n",
        "\n",
        "# === MAIN ===\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Preparing dataset...\")\n",
        "    prepare_dataset()\n",
        "    print(\"T                                                                                                                                                                                                                                                      raining and comparing models...\")\n",
        "    train_and_compare()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}